#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_amsmath 1
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Graphical Models: Take Home Exam
\end_layout

\begin_layout Author
Mohamed N'AITN'BARK
\end_layout

\begin_layout Section
Bayesian least squares regression
\end_layout

\begin_layout Subsection*
(a)
\end_layout

\begin_layout Paragraph
Let 
\begin_inset Formula $H=\begin{array}{cccc}
\eta_{1} & 0 & . & 0\\
0 & \eta_{2} & 0 & .\\
. & 0 & . & 0\\
0 & . & 0 & \eta_{d}
\end{array}$
\end_inset

, we have 
\begin_inset Formula $\omega\sim N(0,H)$
\end_inset

 and 
\begin_inset Formula $Y\sim N(X\omega,\sigma^{2}\mathbf{I}_{n})$
\end_inset


\end_layout

\begin_layout Paragraph
hence 
\begin_inset Formula $P(Y\mathbf{,\omega})\mathbf{=}P(Y|\omega)P(\omega)=\frac{1}{(2\pi)^{(n+d)/2}\sigma^{n}\sqrt{\prod_{j=1..d}\eta_{j}}}\exp(-\frac{||Y-X\omega||^{2}}{2\sigma^{2}})\exp(-\frac{\omega^{T}H^{-1}\omega}{2})$
\end_inset


\end_layout

\begin_layout Subsection*
(b)
\end_layout

\begin_layout Paragraph*
The marginal probability of 
\begin_inset Formula $Y$
\end_inset

 is given by:
\end_layout

\begin_layout Paragraph
\begin_inset Formula $P(Y)=\int_{R^{d}}P(Y,\omega)d\omega=\frac{1}{(2\pi)^{(n+d)/2}\sigma^{n}\sqrt{\prod_{j=1..d}\eta_{j}}}\int_{R^{d}}\exp(-\frac{1}{2\sigma^{2}}\left[||Y||^{2}+\omega^{T}(X^{T}X+\sigma^{2}H^{-1})\omega-2Y^{T}X\omega\right])d\omega$
\end_inset

 (1)
\end_layout

\begin_layout Paragraph
Let 
\begin_inset Formula $S=X^{T}X+\sigma^{2}H^{-1}$
\end_inset

and 
\begin_inset Formula $u=S^{-1}X^{T}Y$
\end_inset

, 
\begin_inset Formula $S$
\end_inset

 is inversible because it is a symetric positive definite matrix.
 Using this notation we can easily see that: 
\begin_inset Formula $||Y||^{2}+\omega^{T}(X^{T}X+\sigma^{2}H^{-1})\omega-2Y^{T}X\omega=(\omega-u)^{T}S(\omega-u)+||Y||^{2}-u^{T}Su$
\end_inset

.
 This expression simplifies the integration of (1) by factorizing the exponentia
l:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
P(Y)=\frac{\exp(-\frac{1}{2\sigma^{2}}(||Y||^{2}-u^{T}Su))}{(2\pi)^{(n+d)/2}\sigma^{n}\sqrt{\prod_{j=1..d}\eta_{j}}}\int_{R^{d}}\exp(-\frac{1}{2\sigma^{2}}(\omega-u)^{T}S(\omega-u))d\omega
\]

\end_inset


\end_layout

\begin_layout Paragraph
Knowing that:
\end_layout

\begin_layout Itemize
\begin_inset Formula $u^{T}Su=Y^{T}XS^{-1}X^{T}Y$
\end_inset

 
\end_layout

\begin_layout Itemize

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
\begin_inset Formula $\int_{R^{d}}\exp(-\frac{1}{2\sigma^{2}}(\omega-u)^{T}S(\omega-u))d\omega=\frac{(2\pi)^{d/2}\sigma^{d}}{\sqrt{det(S)}}$
\end_inset

.
 
\end_layout

\begin_layout Paragraph

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
we derive the marginal probability of 
\begin_inset Formula $Y$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
P(Y)=\frac{1}{(2\pi)^{n/2}\sigma^{n-d}\sqrt{\prod_{j=1..d}\eta_{j}det(S)}}\exp(-\frac{1}{2\sigma^{2}}Y^{T}(\mathbf{I_{n}-}XS^{-1}X^{T})Y)
\]

\end_inset


\end_layout

\begin_layout Standard
We can easily verify that:
\begin_inset Formula $(\frac{\mathbf{I_{n}-}XS^{-1}X^{T}}{\sigma^{2}})^{-1}=\sigma^{2}\mathbf{I_{n}}+XHX^{T}$
\end_inset


\end_layout

\begin_layout Standard
Hence 
\begin_inset Formula $Y\sim N(0,\sigma^{2}\mathbf{I_{n}}+XHX^{T})$
\end_inset

.
\end_layout

\begin_layout Subsection*
(c) 
\end_layout

\begin_layout Paragraph
\begin_inset Formula $\log P(Y)=-\frac{1}{2\sigma^{2}}Y^{T}(\sigma^{2}\mathbf{I_{n}}+XHX^{T})^{-1}Y-\frac{n}{2}\log(2\pi)-\frac{1}{2}\sum_{j=1..d}\log(\eta_{j})-\frac{1}{2}\log(det(S))-(n-d)\log(\sigma)$
\end_inset

.
\end_layout

\begin_layout Subsection*
(d)
\end_layout

\begin_layout Standard
\begin_inset Formula $P(\omega|Y)=\frac{P(Y,\omega)}{P(Y)}=\frac{\sqrt{det(S)}}{(2\pi)^{d/2}\sigma^{d}}\exp(-\frac{1}{2\sigma^{2}}\left[||Y-X\omega||^{2}+\sigma^{2}\omega^{T}H^{-1}\omega-Y^{T}(\mathbf{I_{n}-}XS^{-1}X^{T})Y\right])$
\end_inset

.
\end_layout

\begin_layout Standard
Using the same derivations as in (b): 
\begin_inset Formula $u=(X^{T}X+\sigma^{2}H^{-1})^{-1}X^{T}Y$
\end_inset

 and 
\begin_inset Formula $S=X^{T}X+\sigma^{2}H^{-1}$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Formula $P(\omega|Y)=\frac{\sqrt{det(S)}}{(2\pi)^{d/2}\sigma^{d}}\exp(-\frac{1}{2\sigma^{2}}(\omega-u)^{T}S(\omega-u))\sim N(u,\sigma^{2}S^{-1})$
\end_inset

.
\end_layout

\begin_layout Subsection*
(e)
\end_layout

\begin_layout Standard
At the k-th iteration of the EM algorithm, we need to compute 
\begin_inset Formula $P(\omega|Y,\sigma_{k},\eta^{k})$
\end_inset

which is done in (d).
 
\end_layout

\begin_layout Standard
Using (a) and (b), we derive complete likelihood of the problem:
\end_layout

\begin_layout Standard
\begin_inset Formula $l_{c}(Y,\omega,\sigma,\eta)=-\frac{n+d}{2}\log(2\pi)-n\log(\sigma)-\frac{1}{2}\sum_{j=1}^{d}\log(\eta_{j})-\frac{1}{2}\omega^{T}H^{-1}\omega-\frac{1}{2\sigma^{2}}||Y-X\omega||^{2}$
\end_inset

.
\end_layout

\begin_layout Standard
The E-Step corresponds to derive the expecation of the above quantity with
 respect to 
\begin_inset Formula $P(\omega|Y,\sigma_{k},\eta^{k})$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
E_{\omega|Y,k}[l_{c}(Y,\omega,\sigma,\eta)] & = & Cste-\frac{n}{2}\log(\sigma^{2})-\frac{1}{2}\sum_{j=1}^{d}\log(\eta_{j})-\frac{1}{2\sigma^{2}}E_{\omega|Y,k}[||Y-X\omega||^{2}]-\frac{1}{2}E_{\omega|Y,k}[\omega^{T}H^{-1}\omega]\\
 & = & Cste-\frac{n}{2}\log(\sigma^{2})-\frac{1}{2}\sum_{j=1}^{d}\log(\eta_{j})-\frac{1}{2\sigma^{2}}(||Y-Xu_{k}||^{2}+tr(X^{T}S_{k}^{-1}X))\\
 &  & -\frac{1}{2}(u_{k}^{T}H^{-1}u_{k}+tr(S_{k}^{-1}H^{-1}))
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
The M-Step:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\frac{\partial E_{\omega|Y,k}[l_{c}(Y,\omega,\sigma,\eta)]}{\partial\sigma^{2}} & = & -\frac{n}{2\sigma^{2}}+\frac{1}{2\sigma^{4}}(||Y-Xu_{k}||^{2}+tr(X^{T}S_{k}^{-1}X))\\
\frac{\partial E_{\omega|Y,k}[l_{c}(Y,\omega,\sigma,\eta)]}{\partial\eta_{i}} & = & -\frac{1}{2\eta_{i}}+\frac{(u_{k}^{(i)})^{2}}{2\eta_{i}^{2}}+\frac{(S_{k}^{-1})_{ii}}{2\eta_{i}^{2}}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Since 
\begin_inset Formula $E_{\omega|Y,k}[l_{c}(Y,\omega,\sigma,\eta)]$
\end_inset

 is a convex function with resprect to 
\begin_inset Formula $\sigma^{2}$
\end_inset

and 
\begin_inset Formula $\eta_{j}\,\forall j\in[1,d]$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\sigma_{k+1}^{2} & = & \frac{||Y-Xu_{k}||^{2}+tr(X^{T}S_{k}^{-1}X)}{n}\\
\eta_{i}^{k+1} & = & (u_{k}^{(i)})^{2}+(S_{k}^{-1})_{ii}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Subsection*
(f)
\end_layout

\begin_layout Standard
If some of the parameters 
\begin_inset Formula $\eta_{j}$
\end_inset

are equal to zero, which can be translated in the EM algorithm by a convergence
 toward 
\begin_inset Formula $0$
\end_inset

, the model achieves a dimensionality reduction.
\end_layout

\begin_layout Section*
2 Learning graphical model structures
\end_layout

\begin_layout Subsection*
(a)
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $q$
\end_inset

 be a distribution that factorizes according to G, the KL divergence of
 
\begin_inset Formula $q$
\end_inset

with 
\begin_inset Formula $p$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Formula $D(p||q)=\sum_{X}p(X)\ln(\frac{p(X)}{q(X)})=\sum_{X}p(X)\ln(p(X))-\sum_{X}p(X)\ln(q(X))$
\end_inset

.
\end_layout

\begin_layout Standard
Hence 
\begin_inset Formula $argmin_{q}D(p||q)=argmax_{q}\sum_{X}p(X)\ln(q(X))$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\sum_{X}p(X)\ln(q(X)) & = & \sum_{X}p(X)\ln(\prod_{i=1}^{d}q(X_{i}|X_{\pi(i)}))=\sum_{X}\sum_{i=1}^{d}p(X)\ln(q(X_{i}|X_{\pi(i)}))\\
 &  & =\sum_{i=1}^{d}\sum_{X}p(X)\ln(q(X_{i}|X_{\pi(i)}))\\
 &  & =\sum_{i=1}^{d}\sum_{X_{i},X_{\pi(i)}}\sum_{X_{j}|j\ne i,\pi(i)}p(X)\ln(q(X_{i}|X_{\pi(i)}))\\
 &  & =\sum_{i=1}^{d}\sum_{X_{i},X_{\pi(i)}}p(X_{i},X_{\pi(i)})\ln(q(X_{i}|X_{\pi(i)}))
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
In order to resolve our problem we derive the langrangian:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
L(q,\lambda)=\sum_{i=1}^{d}\sum_{X_{i},X_{\pi(i)}}p(X_{i},X_{\pi(i)})\ln(q(X_{i}|X_{\pi(i)}))+\sum_{i=1}^{d}\sum_{X_{\pi(i)}}\lambda_{X_{\pi(i)}}^{i}(\sum_{X_{i}}q(X_{i}|X_{\pi(i)})-1)
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $L$
\end_inset

is a concave function, hence 
\begin_inset Formula $argmax_{q,\lambda}L$
\end_inset

exists and it is unique.
 The first order conditions are:
\end_layout

\begin_layout Standard
\begin_inset Formula $\forall q(X_{i}|X_{\pi(i)})\neq0:\,\,\frac{\partial L}{\partial q(X_{i}|X_{\pi(i)})}=\frac{p(X_{i},X_{\pi(i)})}{q(X_{i}|X_{\pi(i)})}+\lambda_{X_{\pi(i)}}^{i}=0$
\end_inset

 and 
\begin_inset Formula $\frac{\partial L}{\partial\lambda_{X_{\pi(i)}}^{i}}=\sum_{X_{i}}q(X_{i}|X_{\pi(i)})-1=0$
\end_inset

.
\end_layout

\begin_layout Standard
Hence 
\begin_inset Formula $q_{G}^{*}(X_{i}|X_{\pi(i)})=\begin{cases}
\frac{p(X_{i},X_{\pi(i)})}{p(X_{\pi(i)})} & if\, p(X_{\pi(i)})\neq0\\
0 & if\, p(X_{\pi(i)})=0
\end{cases}$
\end_inset

 and:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
q_{G}^{*}(X)=\prod_{i=1,p(X_{\pi(i)})\neq0}^{d}\frac{p(X_{i},X_{\pi(i)})}{p(X_{\pi(i)})}
\]

\end_inset


\end_layout

\begin_layout Subsection*
(b)
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
D(p||q_{G}^{*}) & = & \sum_{X}p(X)\ln(p(X))-\sum_{X}p(X)\ln(q(X))\\
 & = & \sum_{X}p(X)\ln(p(X))-\sum_{X}p(X)\ln(p(X_{i},X_{\pi(i)}))+\sum_{X}p(X)\ln(p(X_{\pi(i)}))\\
 & = & -H(\{1,..,k\})+\sum_{i=1}^{d}[H(\{i\}\cup\pi(i))-H(\pi(i))]
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
With the convention 
\begin_inset Formula $H(\emptyset)=0$
\end_inset

.
\end_layout

\begin_layout Subsection*
(c)
\end_layout

\begin_layout Standard
The maximum likelihood distribution among all distributions factorizing
 in a given graph G is the distribution 
\begin_inset Formula $q^{*}$
\end_inset

 that minimizes the KL divergence 
\begin_inset Formula $D(p^{\mathcircumflex}||q)$
\end_inset

 where 
\begin_inset Formula $p^{\mathcircumflex}$
\end_inset

is the empirical distribution of 
\begin_inset Formula $X$
\end_inset

.
 Hence 
\begin_inset Formula $q_{G}^{*}(X)=\prod_{i=1,p^{\mathcircumflex}(X_{\pi(i)})\neq0}^{d}\frac{p^{\mathcircumflex}(X_{i},X_{\pi(i)})}{p^{\mathcircumflex}(X_{\pi(i)})}$
\end_inset

.
\end_layout

\begin_layout Standard
Knowing that 
\begin_inset Formula $p^{\mathcircumflex}(X)=\frac{1}{n}\sum_{i=1}^{n}\delta(X-X^{i})$
\end_inset

: 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
q_{G}^{*}(X)=\prod_{i=1,\sum_{j=1}^{n}\delta(X_{\pi(i)}^{j}-X_{\pi(i)})\neq0}^{d}\frac{\sum_{j=1}^{n}\delta(X_{i}^{j}-X_{i})\delta(X_{\pi(i)}^{j}-X_{\pi(i)})}{\sum_{j=1}^{n}\delta(X_{\pi(i)}^{j}-X_{\pi(i)})}
\]

\end_inset


\end_layout

\begin_layout Subsection*
(d)
\end_layout

\begin_layout Standard
Graphs that maximizes the likelihhod are the graphs 
\begin_inset Formula $G$
\end_inset

 such that 
\begin_inset Formula $q_{G}^{*}=p^{\mathcircumflex}$
\end_inset

ie 
\begin_inset Formula $D(p^{\mathcircumflex}||q_{G}^{*})=0$
\end_inset

.
 Hence the set of DAGs that maximizes the likelihood is:
\end_layout

\begin_layout Standard
\begin_inset Formula $\{G\, DAG\,|\,\sum_{i=1}^{d}[H(\{i\}\cup\pi_{G}(i))-H(\pi_{G}(i))]=H(\{1,..,k\})\}$
\end_inset

.
\end_layout

\begin_layout Subsection*
(e)
\end_layout

\begin_layout Standard
For a given graph 
\begin_inset Formula $G$
\end_inset

we need to specify the parameters of each of the conditional distributions
 
\begin_inset Formula $p(X_{i}|X_{\pi(i)})$
\end_inset

, since thos are discrete distributions, for each configuration of 
\begin_inset Formula $X_{\pi_{G}(i)}$
\end_inset

we need 
\begin_inset Formula $k-1$
\end_inset

parameters to specify the conditional distribution.
 Hence:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
c(G)=\sum_{i=1}^{d}k^{|\pi_{G}(i)|}(k-1)
\]

\end_inset


\end_layout

\begin_layout Subsection*
(f)
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $G=(V,E)$
\end_inset

and let
\begin_inset Formula $(i,j)\in E$
\end_inset

 an edge not involved in any v-structure.
 this means that 
\begin_inset Formula $|\pi_{G}(i)|=0$
\end_inset

and 
\begin_inset Formula $|\pi_{G}(j)|=1$
\end_inset

.
\end_layout

\begin_layout Standard
If we consider now 
\begin_inset Formula $G^{'}=(E^{'},V)$
\end_inset

such that 
\begin_inset Formula $E^{'}=(E-\{(i,j)\})\cup\{(j,i)\}$
\end_inset

we have 
\begin_inset Formula $|\pi_{G^{'}}(i)|=1$
\end_inset

and 
\begin_inset Formula $|\pi_{G^{'}}(j)|=0$
\end_inset

.
\end_layout

\begin_layout Standard
Hence 
\begin_inset Formula $k^{|\pi_{G}(i)|}(k-1)+k^{|\pi_{G}(i)|}(k-1)=k^{|\pi_{G^{'}}(i)|}(k-1)+k^{|\pi_{G^{'}}(i)|}(k-1)$
\end_inset

 and finally: 
\begin_inset Formula $c(G)=c(G^{'})$
\end_inset

.
\end_layout

\begin_layout Section*
3 HMM - Implementation
\end_layout

\begin_layout Subsection*
(1)
\end_layout

\begin_layout Standard
The 
\begin_inset Formula $\alpha$
\end_inset

and 
\begin_inset Formula $\beta$
\end_inset

are implemented using the recursion described in the course, we can model
 them in matlab as vectors of size 
\begin_inset Formula $(4,500)$
\end_inset

as we have 
\begin_inset Formula $4$
\end_inset

possible states and 
\begin_inset Formula $500$
\end_inset

data point.
\end_layout

\begin_layout Standard
To have some numercial stability, we use logarithmic values to compute 
\begin_inset Formula $\alpha$
\end_inset

and 
\begin_inset Formula $\beta$
\end_inset

, the 
\begin_inset Formula $logsum$
\end_inset

function computes the log of the sum of an array of probabilities given
 in the logarithmic scale.
\end_layout

\begin_layout Subsection*
(2)
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename fig1.eps
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Marginal probability for each data point in the training set to be in state
 1 (top left), state 2 (top right), state 3 (bottom left) and state 4 (bottom
 right).
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection*
(3)
\end_layout

\begin_layout Standard
The complete log-likelihood of the model is:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
l_{c}(\mathbf{q},\mathbf{u}) & = & \log(p(q_{0})\prod_{t=0}^{T-1}p(q_{t+1}|q_{t})\prod_{t=0}^{T}p(u_{t}|q_{t}))\\
 & = & \log(\prod_{i=1}^{4}\pi_{i}^{\delta(q_{0}=i)}\prod_{t=0}^{T-1}\prod_{i,j=1}^{4}A_{ij}^{\delta(q_{t+1}=j,q_{t+1}=i)}\prod_{t=0}^{T}\prod_{i=1}^{4}N_{k}(u_{t})^{\delta(q_{t}=i)})\\
 & = & \sum_{i=1}^{4}\delta(q_{0}=i)\log(\pi_{i})+\sum_{t=0}^{T-1}\sum_{i,j=1}^{4}\delta(q_{t+1}=j,q_{t}=i)\log(A_{ij})-\\
 &  & \frac{1}{2}\sum_{t=0}^{T}\sum_{i=1}^{4}\delta(q_{t}=i)(\log(|\Sigma_{i}|)+(u_{t}-\mu_{i})^{T}\Sigma_{i}^{-1}(u_{t}-\mu_{i}))+Cste
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
The E step consists of deriving 
\begin_inset Formula $E_{q|u,k}[l_{c}(\mathbf{q},\mathbf{u})]$
\end_inset

, if we denote at the k-th interation:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
E_{q|u,k}[l_{c}(\mathbf{q},\mathbf{u})] & = & \sum_{i=1}^{4}P^{k}(q_{0}=i|\mathbf{u})\log(\pi_{i})+\sum_{t=0}^{T-1}\sum_{i,j=1}^{4}P^{k}(q_{t+1}=j,q_{t}=i|\mathbf{u})\log(A_{ij})-\\
 &  & \frac{1}{2}\sum_{t=0}^{T}\sum_{i=1}^{4}P^{k}(q_{t}=i|\mathbf{u})(\log(|\Sigma_{i}|)+(u_{t}-\mu_{i})^{T}(\Sigma_{i})^{-1}(u_{t}-\mu_{i}))+Cste
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
M step:
\end_layout

\begin_layout Standard
For 
\begin_inset Formula $(\pi_{i})_{1\leq i\leq4}$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula $L(\pi,\lambda)=E_{q|u,k}[l_{c}(\mathbf{q},\mathbf{u})]+\lambda(\sum_{i}\pi_{i}-1)$
\end_inset

is a concave function, hence:
\end_layout

\begin_layout Standard
\begin_inset Formula $\frac{\partial L}{\partial\pi_{i}}=\frac{P^{k}(q_{0}=i|\mathbf{u})}{\pi_{i}}+\lambda=0$
\end_inset

.
 The linear constraint 
\begin_inset Formula $\sum_{i}\pi_{i}=1$
\end_inset

gives the values: 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\forall i\in[1,4]:\pi_{i}^{k+1}=P^{k}(q_{0}=i|\mathbf{u})
\]

\end_inset


\end_layout

\begin_layout Standard
For the matrix 
\begin_inset Formula $\mathbf{A}$
\end_inset

: 
\end_layout

\begin_layout Standard
\begin_inset Formula $L((A_{ij})_{1\leq j\leq4},\lambda)=E_{q|u,k}[l_{c}(\mathbf{q},\mathbf{u})]+\lambda(\sum_{j}A_{ij}-1)$
\end_inset

the lagrangian is a concave function, hence:
\end_layout

\begin_layout Standard
\begin_inset Formula $\frac{\partial L}{\partial A_{ij}}=\frac{\sum_{t=0}^{T-1}P^{k}(q_{t+1}=j,q_{t}=i|\mathbf{u})}{A_{ij}}+\lambda=0$
\end_inset

.
 The linear constraint 
\begin_inset Formula $\sum_{j}A_{ij}=1$
\end_inset

gives: 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
A_{ij}^{k+1}=\frac{\sum_{t=0}^{T-1}P^{k}(q_{t+1}=j,q_{t}=i|\mathbf{u})}{\sum_{t=0}^{T-1}\sum_{j^{'}=1}^{4}P^{k}(q_{t+1}=j^{'},q_{t}=i|\mathbf{u})}
\]

\end_inset


\end_layout

\begin_layout Standard
For the means of the gaussians 
\begin_inset Formula $(\mu_{i})_{1\leq i\leq4}$
\end_inset

:
\end_layout

\begin_layout Standard

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
\begin_inset Formula $E_{q|u,k}[l_{c}(\mathbf{q},\mathbf{u})]$
\end_inset

 is a concave function of 
\begin_inset Formula $\mu_{i}$
\end_inset

therefore we need to verify the first order condition:
\end_layout

\begin_layout Standard
\begin_inset Formula $\nabla_{\mu_{i}}E_{q|u,k}[l_{c}(\mathbf{q},\mathbf{u})]=\sum_{t=0}^{T}P^{k}(q_{t}=i|\mathbf{u})(\Sigma_{i}^{k})^{-1}(u_{t}-\mu_{i}^{k})=0$
\end_inset

.
 Hence:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\mu_{i}^{k+1}=\frac{\sum_{t=\text{0}}^{T}P^{k}(q_{t}=i|\mathbf{u}).u_{t}}{\sum_{t=\text{0}}^{T}P^{k}(q_{t}=i|\mathbf{u})}
\]

\end_inset


\end_layout

\begin_layout Standard
For the covariance matrix 
\begin_inset Formula $(\Sigma_{i})_{1\leq i\leq4}$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula $\nabla_{\Sigma_{i}}E_{q|u,k}[l_{c}(\mathbf{q},\mathbf{u})]=\sum_{t=0}^{T}P^{k}(q_{t}=i|\mathbf{u})(\Sigma_{i}^{-1}((u_{t}-\mu_{i})(u_{t}-\mu_{i})^{T}-\mathbf{I_{4}}))\Sigma_{i}^{-1}$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $E_{q|u,k}[l_{c}(\mathbf{q},\mathbf{u})]$
\end_inset

 is a concave function of 
\begin_inset Formula $\Sigma_{i}$
\end_inset

, hence:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\Sigma_{i}^{k+1}=\frac{\sum_{t=0}^{T}P^{k}(q_{t}=i|\mathbf{u})(u_{t}-\mu_{i}^{k})(u_{t}-\mu_{i}^{k})^{T}}{\sum_{t=0}^{T}P^{k}(q_{t}=i|\mathbf{u})}
\]

\end_inset


\end_layout

\begin_layout Subsection*
(4)
\end_layout

\begin_layout Standard
After the implementation of the EM algorithm we get the following results:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\pi & = & [1,0,0,0]\\
A & = & \begin{bmatrix}0.8935 & \,\,0.0066 & \,\,0.0743 & \,\,0.0256\\
0.0638 & \,\,0.0158 & \,\,0.8768 & \,\,0.0436\\
0.0419 & \,\,0.9166 & \,\,0.0338 & \,\,0.0077\\
0.0329 & \,\,0.0501 & \,\,0.0383 & \,\,0.8786
\end{bmatrix}\\
\mu_{1} & = & [-3.0284\,,-3.5013]\\
\mu_{2} & = & [-2.0354\,,\,4.1934]\\
\mu_{3} & = & [3.9906\,,\,3.8908]\\
\mu_{4} & = & [3.7980\,,-3.8734]
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Subsection*
(5)
\end_layout

\begin_layout Standard
We plot the loglikelihood as a function of iterations of the algorithm:
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align left
\begin_inset Graphics
	filename fig2.eps
	scale 40

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Log likelihood vs EM iterations in the training (left) and test (right)
 sets.
 The curves suggest that we have convergence.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection*
(6)
\end_layout

\begin_layout Standard
\begin_inset Float table
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="3" columns="3">
<features tabularvalignment="middle">
<column alignment="center" valignment="top" width="0">
<column alignment="center" valignment="top" width="0">
<column alignment="center" valignment="top" width="0">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Gaussian mixture model
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Hidden markov model
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
EMGaussienne.data
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
-2327.7
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
-980.6503
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
EMGaussienne.test
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
-2409
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
-1040.3
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Log likelihoods for the Gaussian Mixture model and the HMM model for training
 and test data
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
As expected the HMM model fits more the data on the training set, this is
 due to the fact that that the HMM model is a rich version of the gaussian
 mixture model, the graph of GMM is a set of the graph of the HMM.
 The likelihoods values on the test sets shows that the HMM model captures
 the structure of data better than the GMM.
\end_layout

\begin_layout Subsection*
(7)
\end_layout

\begin_layout Standard
We implement the Viterbi algorithm that consists of computing a Max-Product
 version of 
\begin_inset Formula $\alpha_{t}$
\end_inset

and remebring at each step the state that leads to the most likely sequence.
 Once the last state found we use the memorized transitions to deduce the
 most likely sequence.
 Applied to the data, we derive a classification of the training set.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename fig3.eps
	scale 40

\end_inset


\begin_inset space \hfill{}
\end_inset


\begin_inset Graphics
	filename fig4.eps
	scale 40

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
The classification of the data points is the result of the Viterbi algorithm
 on the training set (left) and test set (right)
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection*
(8)
\end_layout

\begin_layout Standard
We apply the same recursions as in question (2) (alpha beta sum product)
 to compute the marginal probabilities 
\begin_inset Formula $P(q_{t}|u_{1},...,u_{T})$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename fig5.eps
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Marginal probability for each data point in the test set to be in state
 1 (top left), state 2 (top right), state 3 (bottom left) and state 4 (bottom
 right).
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection*
(9)
\end_layout

\begin_layout Standard
In this question we plot for each data point in the test set in the temporal
 order the state that maximizes the marginal probability:
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename fig6.eps
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Most likely state vs time for the first 100 data points.
 The most likely state is computed by maximizing the marginal probability.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection*
(10)
\end_layout

\begin_layout Standard
Now we compare the Viterbi decoding and the approach of the previous question.
 The two approaches give the same results for 
\begin_inset Formula $98\%$
\end_inset

of the data points (
\begin_inset Formula $490$
\end_inset

of 
\begin_inset Formula $500$
\end_inset

).
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename fig7.eps
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
The most likely sequence given by the Viterbi decoding (+) and the most
 likely states given by maximizing the marginal probabilities vs time for
 the first 100 data points.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection*
(11)
\end_layout

\end_body
\end_document
